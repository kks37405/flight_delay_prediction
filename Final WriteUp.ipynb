{"cells":[{"cell_type":"markdown","source":["# w261 Final Project - Airline Flight Delay Prediction"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c7f1bca-b9c6-47f3-b2a7-2329a8c715be"}}},{"cell_type":"markdown","source":["# Table of Contents\n\n__Section 1__ - Question Formulation\n\n__Section 2__ - EDA & Discussion of Challenges\n\n__Section 3__ - Feature Engineering\n\n__Section 4__ - Algorithm Exploration\n\n__Section 5__ - Algorithm Implementation\n\n__Section 6__ - Conclusions\n\n__Section 7__ - Application of Course Concepts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb208d7e-034f-4fb1-9ffb-21f5877e731e"}}},{"cell_type":"markdown","source":["##Section 1 - Question Formulation\n\nFrom the NEXTOR *Total Delay Impact Study: A Comprehensive Assessment of the Costs and Impacts of Flight Delay in the United States* we can start to draw a picture of the heavy economic impact air travel delays can have. The study analyzed all US domestic flights in 2007 and found that total cost to the United States was estimated to be around $31.2 billion dollars. The costs are primarily attributed to the direct loss of profits and increased expenditures attributed to managing personnel, rerouting flights, maintenance, flight cancellations, etc. However, the existence of delays also have significant estimated welfare loss due to lowered demand as well as indirect trickle-down econommic impact on limiting potential travel and secondary industry stimulation (hospitality, entertainment, etc.). \n\nMachine learning algorithms have become much more extensively used to optimize airline and airport operations after evaluating factors impacting flight delays. Pyrgiotis et al. modeled propagation within an airport network, measuing impact of delays from one airport to another. The model iterated across (1) a queuing model computing delays at individual airports, and (2) delay propagation algorithm which continuously updates schedules and demand to all airport in the network off of the queuing model. Simaiakis et al. looked into potentially reducing taxi-times by modeling a queuing system for departing aircraft. Their work was further extended and developed using more robust datasets and statistical methods by Clewlow et al. More recently, Gopalakrishnan et al. compared various machine learning algorithms in prediction delayed flights -- though failed to consider neural networks and decision trees. \n\nWhile the primary stakeholders can be considered entities such as the Federal Aviation Administration (FAA), we can also consider traveling clientele and even federal agencies to have a key interest in accurate airline delay predictions. Mitigating the economic losses from unexpected delays will alleviate costs incurred by the airlines, increase customer satisfaction, and result in positive externalities thanks to efficient shipping of goods/services/individuals. Optimizing flight operations will also help mitigate the vast amounts of pollution created from aircraft -- taxi-out operations are responsible for 45,000 tons of carbon monoxide emissions in the US (2007).    \n\n![](https://www.researchgate.net/profile/Jorge-Soares-12/publication/315382748/figure/fig4/AS:649756581826563@1531925449831/Data-model-of-the-flight-delay-prediction.png)\n\n***\n\n####Project Goal:   \n>Predict flight departure delays two hours prior of scheduled departure (where a delay is defined as a minimum 15-minute delay with respect to the planned departure time)\n\n**Questions to Consider:**\n- What variables are known to us two hours prior to scheduled departure? (Avoid using highly correlated variables such as `DELAY` related columns as they bleed out of the allowed time frame)\n- How shall we address messy/missing values?\n- Are these high impact attributes impactful or simply contributing to data leakage?\n- What additional insights/features can we draw out of the available data?\n\nModel accuracy will help us determine how well our model does in classifying airlines as delayed or not delayed. However, knowing that our classes are imbalanced (majority not delayed flights) we will need to take good care to check metrics of precision and recall especially for delayed flights. The emphasis is on correctly identifying delayed flights and it would not be productive if we (worst case) had a model that predicted on-time for every single flight. (This would actually give us a model accuracy of 77.6%).  A meaningful value of precision and recall would be >80%, where delayed flights are correctly predicted for the most part and airlines could make the appropriate manuevers to mitigate the issue.\n\n***\n\n*Citations:   \nM. Ball, et al. \"Total Delay Impact Study: A Comprehensive Assessment of the Costs and Impacts of Flight Delay in the United States.\" 2010   \nR. Clewlow, I. Simaiakis, and H. Balakrishnan, “Impact of arrivals on departure taxi operations at airports,” 2010.   \nN. Pyrgiotis, K. M. Malone, and A. Odoni, “Modelling delay propagation within an airport network,” Transportation Research Part C: Emerging Technologies, vol. 27, pp. 60–75, 2013.   \nI. Simaiakis and H. Balakrishnan, “Queuing models ofairport departure processes for emissions reduction,” in AIAA Guidance, Navigation and Control Conference and Exhibit, vol. 104, 2009.   \nK. Gopalakrishnan and H. Balakrishnan, “A comparative analysis of models for predicting delays in air traffic networks,” in USA/Europe Air Traffic Management Seminar,2017.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49737603-88a9-4bc7-8e44-4c0c7617e9d4"}}},{"cell_type":"markdown","source":["##Section 2 - EDA & Discussion of Challenges\n\n**Missing Values:** \n\nExplore the available features and cut down on irrelevant ones to reduce the load on our models. Initial analyses of the data indicated that many data points, especially in the weather data set were blank.  The first such pass over the data was carried out by reading in the data, counting blanks per column via user-defined helper function.  Viewing columns having a number of blank values led to many columns being dropped. (See Feature Engineering - Final Data Processing Pipeline)\n\n**Imbalanced Data:**   \nAs noted earlier when exploring our evaluation metric and dependent variable, the available data for our two classes (delayed and not delayed flights) is very unbalanced. This can lead to heavy biases in determining our model efficacy and notions of down/up-sampling are important to consider in our model implementations.\n\n__6 Month__\n![](https://snipboard.io/TJyukY.jpg)\n\n__Full Data__\n![](https://snipboard.io/VraAW5.jpg)\n\n**Distribution of Numeric Values:**\n\nFor the 6 months data, observed the frequency distribution of various numeric variables. Note that this dataset only includes flights originating from two airports (as shown in ORIGIN_AIRPORT_ID and ORIGIN_STATE_FIPS). Among these plots, we can see DEP_DELAY, TAXI_IN, and TAXI_OUT have heavily right-skewed distributions. MONTH, DAY_OF_MONTH, WHEELS_ON, and WHEELS_OFF are fairly normally distributed. ARR_TIME and DEP_TIME are also fairly focused around daylight hours as one would expect.\n\n![](https://snipboard.io/pJYyCE.jpg)\n\n**Temporal Departure Delay Trends:**\n\nFor the temporal metrics that are easily accessible as soon as the flights are scheduled, we plotted the average departure delays across the 6 month data. For the two airports we can see distinct differences in delays between airports (ORD tends to be much higher). There are also distinct differences in temporal/seasonal trends:\n- Months of March-May tended to have the least delays - likely due to less demand and scheduled flights\n- Spikes in Delays tended to be most extreme on weekends and the least on the extreme ends of weekdays\n- Flights prior to 5:00 AM were also the most delayed\n\n![](https://snipboard.io/YtpyHh.jpg)\n\n**Correlation Plots:**   \nIn the construction of linear models, collinearity can introduce instability by making multiple solutions valid.  Consider the process \\\\(y=x_1+x_2\\\\) where \\\\(x_2=2x_1\\\\).  This way, \\\\(y\\\\) can be wholly defined in terms of either \\\\(x_1\\\\) or \\\\(x_2\\\\):\n\n$$ y=x_1+2*x_1 = 3x_1 $$\n$$y=(1/2)x_2+x_2=(3/2)x_2 $$\n\nAssuming ideal data with no noise, each of these equations is an equally valid regression, but the first equation gives the result \\\\((3=x_1, 0=x_2)\\\\) and the second \\\\((0=x_1, 3/2=x_2)\\\\).  However, these two solutions define a line themselves and show that \\\\((1=x_1,1=x_2)\\\\) being on the line is also a solution.  In fact any other point on the line \\\\(x_2=-0.5x_1+3/2\\\\) is also a solution.  The spectrum of solutions on the line makes optimization and finding a unique solution difficult, solutions with great or impossible and this motivates some analysis to find and eliminate collinear data.\n\nTo aid in collinearity detection, a correlation matrix was generated and plotted.  A correlation matrix is generated by taking each of the feature vectors and finding their pairwise correlation values.  The matrix is a square matrix whose rows are the same in number as the number of features.  Additionally, the correlation between features indexed by i and j are placed into the cell indexed by i and j.  A correlation heatmap plot is created by coloring a grid of cells of the matrix according to the correlation values.  We noticed from the plot below that no two features were perfectly correlated.\n\n![](https://snipboard.io/d2TlyK.jpg)\n\n***\n\n#### Additional Scalability Precautions\n\n**Saving Dataset and Caching:**   \nFrequent saving of our datasets as a Parquet file and caching transformed data allowed us to run code efficiently. Caching and saving reduces the need to reload the data each time it is being used. In particular, at the end of our data processing notebook we save our final joined table to parquet for subsequent model iterations and final model runs.\n\n\n[EDA Visuals](https://github.com/kks37405/flight_delay_prediction/blob/main/EDA%20Visuals.ipynb)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb79c398-7e2d-431a-b102-06f1d7d535ba"}}},{"cell_type":"markdown","source":["## Section 3 - Feature Engineering\n\n#### Main Approaches\n**Final Data Pipeline:**\n\nWe built our data processing pipeline in the following manner.\n\nAirlines Table:\n- Drop columns with more than 1/3 null values\n- Drop irrelevant and redundant columns (i.e. Airport name and Airport ID)\n\nWeather Table:\n- Retain only US records \n- Keep columns of weather records for FM15 report type (Aviation routine weather report)\n- Retain relevant columns from un-nested weather data\n  - __Mandatory Data__ and __Additional Data__ section from the FEDERAL CLIMATE COMPLEX DATA DOCUMENTATION were used as a guide to select features from Weather table.\n\n\nJoins:\n1. Join stations table with weather records using concatenated USAF and WBAN codes\n2. Add helper table codes to map airport ICAO and IATA identification systems\n3. Join airlines with weather by taking only the most recent weather station data at least two hours prior to flight planned departure time\n\nMissing Values:\n- Drop rows with missing value for our dependent metric `DEP_DEL15`\n- Data Imputation:\n  - Median - for skewed numeric attributes and discrete variables\n  - Mean - for non-skewed numeric attributes\n\n[Data Processing Pipeline](https://github.com/kks37405/flight_delay_prediction/blob/main/Data%20Preprocessing%20Pipeline.ipynb)\n\n**Feature Selection by L1/L2 Regression:**\n\nL1 and L2 regression, also known as “Lasso” and “Ridge” regression respectively, share the sum of squares of residuals in their cost functions.  However each of L1 and L2 regressions contain an additional distinct term that adds to their costs.  The Lasso cost function contains the term i||and the Ridge cost function contains the term ii2.  Here the  are the weights that are “learned” during a regression’s cost optimization routines.  This way the extra term for each of the L1 and L2 cost functions adds a cost depending on the sum of the absolute values of the weights (for L1) or the sum of their squares (for the L2).  In either case the  parameter is used to tune the regression’s penalization; if zero then OLS is used because the penalty term itself becomes zero.\n\nIn the L1 regression because the sum is used some weights are forced towards zero while others flourish away from zero.  In some cases, it may depend on any collinearity present and initial values in any optimization process. We used our L1 and L2 regression models to guide our feature selection for our final dataset.\n\n[L1/L2 Regression](https://github.com/kks37405/flight_delay_prediction/blob/main/L1%20Regression.ipynb)\n\n**Indexing and Encoding Categorical Values:**\n\nTo handle categorical values in the dataset, we applied indexing and one hot encoding to convert to numeric datatypes suitable for the algorithms.  These transformations were necesssarily depending on the algorithms.  For example, decision trees can handle categorical data while logistic regression could only handle numeric data.\n\n**Down-sampling:**   \nTo balance out the dataset, we applied the approach of downsampling the majority class so the number of observations for on-time flights were roughly even with delayed flights.  Balancing the classes prevents the data from introducing a bias in the models, where it may just simply predict the majority class all the time.  Up-sampling the minority class would have been another approach if we had a very small dataset and each observation would be too valuable to discard.  But given that we had >10 million rows of data, down-sampling was a resource appropriate technique for our problem.\n\n***\n\n#### Alternative Approaches \n\n**Dimensionality Reduction with PCA:**   \nThe essential function of a Principal Component Analysis (PCA) is to take the matrix of data, rearrange and factor it so as to express it in terms of “principal components” which might reveal structure in the data.  In some sense, it’s not unlike considering the integer value 66 but in considering its binary equivalent 1000010 (2^6+2^1=64+2=66).\n\nWhat PCA does is it expresses each datapoint in terms of “principal component vectors” and “principal component values”.  PCA can be interpreted as iteratively seeking vectors who define a span of greatest variability.  Once that vector is defined, the process can be considered as repeating iteratively until a vector is created for each feature dimension.  These “principal vectors” once defined can often reveal vectors which best separate labels.  This is because the vectors are defined by seeking bases of greatest variability.  For such reasons PCA is often used as a dimensionality reduction technique.\n\nWe subjected our weather data to PCA. After that, the resulting PCA vectors were input to a decision tree classifier trained on the data using the labels. We got an accuracy of 78% for a decision tree classifier and 88% with a random forest.\n\n**New Features:**\n- Rolling percentage of flights delayed in an airport that day (excluding current entry)\n- Percentage of flights delayed in an airport that day in the hour block two steps prior to the block the current flight is contained in\n\nBoth of these features for consideration were ultimately excluded because we decided to avoid time-series dependent features. Later on in our algorithm implementation, we opted for a randomized test-train split solely dependent on features easily accessible and available for the model. That way we could still make reasonable predictions even if we applied the classifier on new data (in which case we may have no information on flight delays earlier in the day). \n\n[PCA + New Features](https://github.com/kks37405/flight_delay_prediction/blob/main/PCA%20%2B%20New%20Features.ipynb)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce3dee6-10e5-4d69-bb1c-865f4f510b93"}}},{"cell_type":"markdown","source":["## Section 4 - Algorithm Exploration\n\n**Screening Models:**  \n\nThree tree based algorithms were trained on the 6 month dataset to serve as baselines to select a final algorithm for the full dataset.  The three tree algorithms were: decision tree, random forest, and gradient boosted decision tree.  The algorithms also underwent simple hyperparameter tuning during cross validation.\n\nOf all the algorithms, it was expected that the gradient boosted decision tree would produce the highest score given the model's complexity.  A likely drawback of the gradient boosted decision tree however, is that it can be prone to overfitting or high variance.  A random forest would less likely to overfit than gradient boosted decision tree given its complexity but may have higher bias.  To prevent overfitting, we applied cross validation with hyperparameter tuning to find the model with the best score.\n\n**Choosing our best model:** \n\nThe evaluations from the three models on the 6 month test set is shown below.  In terms of Precision and Recall score, a gradient boosted decision tree performed the best and was chosen for the final algorithm.  A notebook detailing the implementation of the algorithms can be found here: \n\n[Algorithm Exploration](https://github.com/kks37405/flight_delay_prediction/blob/main/Algorithm%20Exploration-6m-UnderSample.ipynb).   \n\n![Precision and Recall](https://snipboard.io/r80Juc.jpg)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d151815d-3d32-4281-a492-6acc685a000c"}}},{"cell_type":"markdown","source":["##Section 5 - Algorithm Implementation\n\n**Decision Trees:**   \nDecision trees classify data like a coin falling through a chute with gates whose sizes are tuned for the various denominations.  Each node in the tree defines an easy to interpret rule which is very helpful to understand the tree’s explainability and how it works and is used in classification. The tree construction process is relatively complex however. It relies on a recursive and “greedy” process to define rules used to classify the data.  The “greedy” term is used to indicate that the same process is used at each branch point’s creation without regard to what may be happening at any other branch or with any other data.  \n\nAs the tree is built the data at each node is subjected to an information-gain analysis based on “impurity” values. The impurity is a measure of data homogeneity. At each branch point, different binary splits are considered.  For each split considered the information gain is calculated as the impurity of the data at the node minus the impurities of the data at each subtree if the split were carried out.  This is detailed as in the equation below: \n$$IG(D,s) = Impurity(D)-\\frac{N_{left}}{N} Impurity(D_{left})-\\frac{N_{right}}{N}Impurity(D_{right}) $$\n\nFor our tree modeling we used the Gini Impurity. In our dataset there were two labels : DEP_DEL15 yes (1) and DEP_DEL15 no (0).  For an example consider 100 records evenly split with 50 labels of each.  If a binary split puts all of “yes” label on one side the remaining “no” we’d have zero impurity on the left, 0 impurity on the right and the branch under consideration would have impurity of 0.5 yielding a gain of 0.5.  In contrast supposing a split were to be non-informative and put 25 “yes” in both the left and right branches as well as 25 “no” in both branches too.  In that case the impurities in each branch would be the same and their weighted sum would equal the total impurity at the root and so information gain would be zero.  The Gini impurity is calculated as below:\n\n$$\\sum^C_{i=1} f_i (1 - f_i) $$\nwhere f is frequency of label i at a node and C is the number of unique labels\n\n**Gradient-Boosted Trees** are ensembles of decision trees, they iteratively train DTs in order to minimize a loss function.\n\n**Random Forests** are ensemble trees, they combine decision trees in order to reduce the risk of overfitting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e332f036-88f7-48fe-9b56-d221aa181011"}}},{"cell_type":"markdown","source":["#### 5a Decision Trees - A Small Toy and Exercise\n\nA process is used to simulate weather that is either \"Sunny\" or \"Not Sunny\".  Temperature is allowed to range from 50 to 100.  If temperature is an odd number then there is precipitation.  Finally, if the temperature is greater than or equal to 75 and if there is no precipitation then the weather is set as sunny.  For demonstration purpposes, an additional non-informative random boolean value \"rand_bool\" is added.  Being random it carries no information and is not expected to help in terms of information gain.\n\nIn our weather dataset we had precipitation data.  One way we could have worked with it in terms of feature engineering is to set it as a binary variable \"Precipitation\" or \"No Precipitation\".  This toy data, having only boolean data therefore presents a hypothetical avenue of that kind of feature engineering."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"072b2b78-8212-49d0-ab5a-1f08aba8337f"}}},{"cell_type":"code","source":["gen_data=list()\nimport random\n\n#temp ranges from 50 to 100\nfor temp in range(50,101):\n    sunny=False\n    temp_geq_75=temp>=75\n    precip=temp%2==0\n    sunny=temp_geq_75 and not(precip)\n    rand_uni=random.random()\n    rand_bool=False\n    if rand_uni>=0.5:\n      rand_bool=True\n    row=[temp_geq_75,precip,rand_bool,sunny]\n    gen_data.append(row)\nimport pandas as pd\n\ntoy_df = pd.DataFrame(gen_data, columns = ['temp_geq_75','precip','rand_bool','sunny'])\nprint(\"The first few records\")\nprint(toy_df.head())\nprint(\"The last few records\")\nprint(toy_df.tail())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bee0890c-bbfa-4554-a23c-43d49b8a57ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The first few records\n   temp_geq_75  precip  rand_bool  sunny\n0        False    True      False  False\n1        False   False       True  False\n2        False    True      False  False\n3        False   False      False  False\n4        False    True      False  False\nThe last few records\n    temp_geq_75  precip  rand_bool  sunny\n46         True    True      False  False\n47         True   False       True   True\n48         True    True       True  False\n49         True   False       True   True\n50         True    True      False  False\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The first few records\n   temp_geq_75  precip  rand_bool  sunny\n0        False    True      False  False\n1        False   False       True  False\n2        False    True      False  False\n3        False   False      False  False\n4        False    True      False  False\nThe last few records\n    temp_geq_75  precip  rand_bool  sunny\n46         True    True      False  False\n47         True   False       True   True\n48         True    True       True  False\n49         True   False       True   True\n50         True    True      False  False\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Tree-Related Routines\n\nA routine to compute Gini-Impurity from a list of booleans is written. A utility to return the most frequent boolean is written. Finally, a function to find information gain if a data frame is split on a column is also provided.  In addition, a recursive routine to generate a decision tree (in the form of a dict) is written which invokes aformention dependent routines. Once a column has been used it cannot be re-used. Once there are no columns to consider, then the result of the tree is the most frequent label seen in the resulting data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7d1d893-203b-49f8-b95e-8af80ad7f8ae"}}},{"cell_type":"code","source":["def calcImpurity(boolean_list):\n    \"\"\"Compute GINI impurity\"\"\"\n    #https://spark.apache.org/docs/2.2.0/mllib-decision-tree.html#node-impurity-and-information-gain\n    num_labels=2\n    true_data=[b for b in boolean_list if b==True]\n    false_data=[b for b in boolean_list if b==False]\n    true_freq=float(len(true_data))/float(len(boolean_list))\n    false_freq=float(len(false_data))/float(len(boolean_list))\n    freqs=[false_freq,true_freq]\n    impurity=sum([f_i*(1.0-f_i) for f_i in freqs])\n    return impurity\n\ndef getMostFrequent(boolean_list):\n    \"\"\"return the most frequent boolean (True or False)\"\"\"\n    count_true=len([b for b in boolean_list if b==True])\n    count_false=len([b for b in boolean_list if b==False])\n    if(count_true>=count_false):\n        return True\n    else:\n        return False\n\ndef colSplitIG(df,col_name,target_col):\n    \"\"\"if a dataframe whose label/target is indicated, but split on the given column , return the information gain\"\"\"\n    #get the splits data structures\n    left_df=df[df[col_name]==False]\n    right_df=df[df[col_name]==True]\n    left_df_labels=left_df[target_col].tolist()\n    right_df_labels=right_df[target_col].tolist()\n    here_labels=df[target_col].tolist()\n    \n    #compute gain\n    impurity_here=calcImpurity(here_labels)\n    #left \n    r_left=float(len(left_df_labels))/float(len(here_labels))\n    imp_left=calcImpurity(left_df_labels)\n    left_term=r_left*imp_left\n    #right\n    r_right=float(len(right_df_labels))/float(len(here_labels))\n    imp_right=calcImpurity(right_df_labels)\n    right_term=r_right*imp_right\n    #final computation\n    info_gain=impurity_here-left_term-right_term\n    return info_gain\n\ndef returnTree(df,target_col,cols_to_consider):\n    \"\"\"recursively implement a decision tree algorithm.  Never re-use a column\"\"\"\n    #the base case for the recursion is when len(cols_to_consider)==0 ; just return the most common target label\n    the_dict=dict()\n    if(len(cols_to_consider)==0):\n        the_dict['return']=str(getMostFrequent(df[target_col].tolist()))\n        return the_dict\n    #in the recurive case, keep track of info. gain depending on the column splits\n    #then once we know what column is used (whose split results in greatest gain)\n    #subtract that column from the list considered. This 'trimmed' list of columnsis passed in \n    #the recursive call as cols_to_consider so that no column is re-used\n    cols_to_skip=list()\n    the_igs=list()\n    #consider splitting on each column and compute the info. gain\n    for col_name in cols_to_consider:\n        ig=colSplitIG(df,col_name,target_col)\n        the_igs.append(ig)\n    #find the column whose split led to max gain and remove it from a list\n    max_ig=max(the_igs)\n    max_ig_idx=the_igs.index(max_ig)\n    max_ig_col_name=cols_to_consider[max_ig_idx]\n    cols_to_skip=[c for c in cols_to_consider if c!=max_ig_col_name]\n    #set up for a recursive vall\n    dict_key_f=str(max_ig_col_name)+'==false'\n    dict_key_t=str(max_ig_col_name)+'==true'\n    left_df=df[df[max_ig_col_name]==False]\n    right_df=df[df[max_ig_col_name]==True]\n    #call recursively and return\n    the_dict[dict_key_f]=returnTree(left_df,target_col,cols_to_skip)\n    the_dict[dict_key_t]=returnTree(right_df,target_col,cols_to_skip)\n    return the_dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3acb9a49-df5c-4e0c-96dd-068405469b3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Build A Tree\n\nUse the code to generate a decision tree!  As seen in the toy tree, the first split is on precipitation which is most informative.  Finally, towards the leaves, the last splits are on the \"rand_bool\" , which, due to its random nature, is least informative.  Its non-informative nature puts its split at the leaves because it gives the least information gain!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dfae590-1354-4d1a-bd94-be8b46a53298"}}},{"cell_type":"code","source":["import json\n#The target column is 'sunny'\ntarget_col='sunny'\n#use the other columns\ncols_to_consider=['temp_geq_75','precip','rand_bool']\n#build and print a tree!\ntoy_tree=returnTree(toy_df,target_col,cols_to_consider)\nprint(json.dumps(toy_tree,indent=4))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dda4564-4a76-4e71-b600-f380ed7bca49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{\n    &#34;precip==false&#34;: {\n        &#34;temp_geq_75==false&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            }\n        },\n        &#34;temp_geq_75==true&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;True&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;True&#34;\n            }\n        }\n    },\n    &#34;precip==true&#34;: {\n        &#34;temp_geq_75==false&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            }\n        },\n        &#34;temp_geq_75==true&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            }\n        }\n    }\n}\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;precip==false&#34;: {\n        &#34;temp_geq_75==false&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            }\n        },\n        &#34;temp_geq_75==true&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;True&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;True&#34;\n            }\n        }\n    },\n    &#34;precip==true&#34;: {\n        &#34;temp_geq_75==false&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            }\n        },\n        &#34;temp_geq_75==true&#34;: {\n            &#34;rand_bool==false&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            },\n            &#34;rand_bool==true&#34;: {\n                &#34;return&#34;: &#34;False&#34;\n            }\n        }\n    }\n}\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Section 6 - Conclusions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7e9a8e2-f302-4efd-9046-a91cde4097a1"}}},{"cell_type":"markdown","source":["__Final Gradient Boosted Tree Result:__ 6 -12 hrs.\n\n[Final Gradient Boost](https://github.com/kks37405/flight_delay_prediction/blob/main/Full-Model.ipynb)\n\n```\n'ROC AUC': 0.7043538319027595, 'PR AUC': 0.6493648690779376\n              precision    recall  f1-score   support\n         0.0       0.66      0.75      0.70   1139216\n         1.0       0.63      0.54      0.58    929962\n    accuracy                           0.65   2069178\n   macro avg       0.65      0.64      0.64   2069178\nweighted avg       0.65      0.65      0.65   2069178\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8da54a03-cb7b-42c6-8f3c-782881f0e28c"}}},{"cell_type":"markdown","source":["|                         Model: Dataset                           |Recall|Precision|ROC AUC|  PR AUC  |\n|------------------------------------------------------------------|------|---------|-------|----------|\n|__DT__ (depth=0, trees=1): __6 month__                            |0.78  |0.603    |0.5    |**0.22**  |\n|__RF__ (depth=6, trees=20): __6 month__                           |0.78  |0.603    |0.68   |**0.40**  |\n|__GBT__ (depth=6, trees=30): __6 month__                          |0.80  |0.772    |0.72   |**0.48**  |\n|__DT__ (depth=10, trees=1327): __6 month Undersample__            |0.66  |0.66     |0.60   |**0.54**  |\n|__RF__ (depth=10, trees=30, bins =80): __6 month Undersample__    |0.65  |0.65     |0.71   |**0.64**  |\n|__GBT__ (depth=10, trees=50): __6 month Undersample__             |0.68  |0.68     |0.74   |**0.66**  |\n|__GBT__ (depth=10, trees=50): __Full Undersample__                |0.65  |0.65     |0.70   |**0.65**  |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38358ecb-ba59-4d95-bf18-d208cb112372"}}},{"cell_type":"markdown","source":["##### Scalability\n\n_Data transformation and pre-processing:_\n- Our data pipline proved to be scalable throughout the transformation and pre-processing workflow (from the 6 month data to the full dataset). The main factor in its scalability was frequent use of saves into parquet, this allowed us to avoid build up of transformation steps. \n- Our join proved very scalable as it completed in a matter of minutes on the smaller 6 month data as well as the full dataset.\n- In order to improve scalability we also reduce the amount of data we transformed from the start by removing unnecessary columns and irrelavent columns. Thus we did not waste resources transforming data that was not needed. \n- We also cached our dataset before running our model to improve our performance when running on a larger dataset.\n\n_ML_\n- When using `CrossValidator` we noticed a major dip in performance even when running on the smaller 6 month dataset. To improve performance we utilized the `parallelism` parameter with `CrossValidator` to set the number of threads to use when running our models in parallel. \n- Early on we used the `pipeline` as the estimator inside `CrossValidator`which executes the entire pipline for every parameter combination fold. With this configuration, our inital Gradient Boosted models took almost half a day to complete. However, for our final model, we set the model specification as the estimator inside the `CrossValidator` since only the model was being tuned in this case and not the `pipeline` to improve performance.\n\n***\n\n*Citations:   \nDatabricks. “How to Speed up Cross-Validation.”  2020, kb.databricks.com/machine-learning/speed-up-cross-validation.html.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33c712e8-301c-4ef9-b56e-a80c15c5cf74"}}},{"cell_type":"markdown","source":["##### Evaluation Metrics\n\n- Recall : Percentage of true positives identified correctly. Out of all the actual 1s, what percent did we predict as 1s.\n$$Recall = \\frac{True Positive}{True Positive + False Negative}$$\n- Precision : Allows us to measure the correct proportion of positive identifications. Out of all our predictions of 1s, what percent were truly 1s.\n$$Precision = \\frac{True Positive}{True Positive + False Positive}$$\n- ROC AUC : Provides insight on how our good our model is at distinguishing between a 0 (Departure delayed less than 15 minutes) and 1 (Departure delayed 15 minutes or more). The higher the ROC AUC, the better the model is at distinguishing between the two.\n- PR AUC : Using precision to provides insight on how many of our predicted positives are true positives. The higher the PR AUC, the better the model is at labeling 0s as 0s and 1s as 1s. This metric is more informative for our case as our data has a high class imbalance between flights between a 0 (Departure delayed less than 15 minutes) and 1 (Departure delayed 15 minutes or more).\n\n\nOur final model, the Gradient Boosted Tree, does a decent job at labeling 0 (Departure delayed less than 15 minutes) identified by its 0.75 recall, but is almost flipping a coin to predict 1 (Departure delayed 15 minutes or more), identified by its 0.54 recall. Our ROC AUC mostly likely as high as it is due the models ability to better predict 0s.\n\nIn terms of precision, our model is just as good at predicting true negatives (0s as 0s) as it is at predicting true positives (1s as 1s). However, the precision is only marginally better than a coin toss. The metrics of our final model are consistent with those that were displayed in its 6 month counter parts. However, The reason why the 6 month Gradient Boosted Tree performs better than the full Gradient Boosted Tree, is due to the difference in imbalance between 6 month and the full data. The 6 month data has 77.6% 0s and 22.4% 1s compared to 81.8% 0s and 18.2% 1s.\n\nEven though most of our metrics for our final have a lower score than its predecessors, our PR AUC is in line with the 6 month data and has highly improved which means compared to the models that came before it (especially ones that did not have undersampling), our final model is the best at predicting 1s (Departure delayed 15 minutes or more). A major factor in our improved targeted measure, PR AUC, is due to our addition of under sampling. Using under sampling we created a closer distribution of labels, 55% 0s and 45% 1s.\n\nGiven the business problem, our model performs poorly at predicting flight delays. Due to the imbalanced labels, our model had a hard time correclty predicting delays. However, we improved our metrics from our initial decision tree has highlighted by the PR AUC score through use of undersampling.\n\n***\n\n*Citations:   \nJ. Davis, M. Goadrich. \"The Relationship Between Precision-Recall and ROC Curves.\" 2006.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f596602d-6b63-413f-bded-98184768f6d2"}}},{"cell_type":"markdown","source":["##### Considerations for Improvement\n- Undersampling helped our final model improve in its label prediction, however, to further improve our model we can consider oversampling. Oversampling is the process of duplicating the minority class, in this case departure delay, in order to have a more even distribution of classes.\n- Imputation during pre-processing was done through custom code. Using Spark's `Imputer` to complete missing values, we can improve scalability. \n- Consider creating new features from our dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7756ff9-77b9-49c7-ac8f-6f944c1ebf7a"}}},{"cell_type":"markdown","source":["## Section 7 - Application of Course Concepts\n\n##### Model Assumptions\n\nDecision trees are non-parametric methods and make no assumptions on the distribution of the underlying data. The same applies for random forests where we have an ensemble of decision trees. These methods can handle skewed and multi-model data, as well as categorical data (both ordinal or non-ordinal).\n\nLogistic regression has various assumptions such as linearity in the logit of continuous variables, no multicollinearity, independent errors, and no strong outliers in the data.\n\n##### Hyperparameter Tuning, Complexity, Bias, and Variance\n\nIn algorithm exploration, we practiced the concepts hyperparameter tuning, complexity, bias and variance in decision trees.  When building the tree algorithms, we had to balance complexity and generalization through hyperparameter tuning of depth (and number of trees in gradient boosted trees).  Though a deeper tree would lead to higher complexity and low bias, it could become prone to overfitting or high variance.  We employed random forests to address the issue of high variance in decision trees however the model complexity was not enough to produce meaningful results.  Ultimately we used gradient boosted trees and prevent overfitting by hyperparameter tuning for the highest score in the validation set.\n\n##### Model training time \n\nTraining the models on the full dataset of > 20 million rows was a time intensive process and to produce results quickly we screened models and features with a smaller dataset.  The strategy of using a smaller dataset first allowed us eliminate models and focus the lengthy training times on a model that would have the most potential.  The stategy also allowed us to perform unit testing on our code before running the full dataset.  A limitation of using a smaller dataset first is that it may not fully represent the full dataset. To accomodate this limitation, we ensured both datasets had a similar distribution of classes.\n\n##### Cluster Managerment\n\nWhile training our dataset and performing cross-validation, performance took a big dip as our cross-validator was executing the entire pipline for every parameter combination fold. When clusters are overloaded, runtime drastically increases turning code that would run in 3 minutes to 21 minutes. This is expounded when running multiple notebooks and models at the same time on the larger datasets. In a certain case when a cluster had running for more than 24 hours, we ran into errors and reduction in performance. Considering cluster health and restarting when it had been active for a long period of time noticeably increase performance.\n\n##### Big Data Piplines\n\nDuring our transformation and pre-processing phase we applied concepts from Big Data Pipelines to maximize performance and scalability. Using Parquet checkpoints kept metadata about the data in order to compute some calculations extremely quickly such as row counts. Use of UDFs were avoided as they are typically much slower than built-in Spark functionality because they need to serialize and deserialize the data for every row that the function is applied to."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8b3f6c9-8096-4fb7-a238-95a507f1dd75"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Final WriteUp","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2185046026060232}},"nbformat":4,"nbformat_minor":0}
